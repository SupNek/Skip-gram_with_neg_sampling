{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc50c85-c1c2-44a3-b096-d36635e0de02",
   "metadata": {},
   "source": [
    "# Skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101286a8-ded3-45d8-8da3-3daf61aab67d",
   "metadata": {},
   "source": [
    "## Оформление проекта в модульном виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f64babe-a3b6-4f20-bb58-ae121e742f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"skip-gram\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbabc24-1886-4d5a-8c7d-7fa34a20d0cc",
   "metadata": {},
   "source": [
    "### 01. Загрузка данных "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf12c21-9cda-4e8f-b0db-dd10f6cd4dc7",
   "metadata": {},
   "source": [
    "Загрузка основного файла `quora.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84dc8d87-ec54-45dd-9add-eaec3d39db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-31 13:23:54--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1 [following]\n",
      "--2025-05-31 13:23:57--  https://www.dropbox.com/scl/fi/p0t2dw6oqs6oxpd6zz534/quora.txt?rlkey=bjupppwua4zmd4elz8octecy9&dl=1\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com/cd/0/inline/Cqt0c8p9awrwnNl-QJ3DSfg2uXtQaVS314z7mDlr3kbvD4V1PlUmc7GzxSxGmQzk4jbRxUsO_JbBT5OmxhMHsorCgugYDeuyWlC4lGWVqqXDNkEHNSO95BnpgM5V_DoksD8/file?dl=1# [following]\n",
      "--2025-05-31 13:23:58--  https://ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com/cd/0/inline/Cqt0c8p9awrwnNl-QJ3DSfg2uXtQaVS314z7mDlr3kbvD4V1PlUmc7GzxSxGmQzk4jbRxUsO_JbBT5OmxhMHsorCgugYDeuyWlC4lGWVqqXDNkEHNSO95BnpgM5V_DoksD8/file?dl=1\n",
      "Resolving ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com (ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com)... 162.125.70.15\n",
      "Connecting to ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com (ucf7487853ad7b3331d7974a6222.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33813903 (32M) [application/binary]\n",
      "Saving to: ‘./data/quora.txt’\n",
      "\n",
      "./data/quora.txt    100%[===================>]  32.25M  10.4MB/s    in 3.5s    \n",
      "\n",
      "2025-05-31 13:24:02 (9.11 MB/s) - ‘./data/quora.txt’ saved [33813903/33813903]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./data/quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bae5f2a-3333-447d-8d64-c7666da9a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import requests\n",
    "\n",
    "# # путь к папке с данными\n",
    "# data_path = Path(\"data/\")\n",
    "\n",
    "# # если папки нет, то она будет создана\n",
    "# if data_path.is_dir():\n",
    "#     print(f\"{data_path} directory exists.\")\n",
    "# else:\n",
    "#     print(f\"Did not find {data_path} directory, creating one...\")\n",
    "#     data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(data_path / \"quora.txt\", \"wb\") as f:\n",
    "#     print(\"Downloading quora.txt...\")\n",
    "#     request = requests.get(\"https://yadi.sk/i/BPQrUu1NaTduEw\")\n",
    "#     f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7702a56b-5b91-4e9c-8439-655f96988954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./data/quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ac66b-5f65-41fb-b4d1-fc21e5c90a92",
   "metadata": {},
   "source": [
    "### 02. Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26353ddf-d689-4c93-9e04-fb93aae7066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting skip-gram/data_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/data_preprocessing.py\n",
    "\"\"\"\n",
    "Contains functionality for data preprocessing.\n",
    "\"\"\"\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def subsample_frequent_words(word_count_dict, threshold=1e-5):\n",
    "    \"\"\"Calculates the subsampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function is used to determine the probability of keeping a word in the dataset\n",
    "    when subsampling frequent words. The method used is inspired by the subsampling approach\n",
    "    in Word2Vec, where each word's frequency affects its probability of being kept.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "    - threshold (float, optional): A threshold parameter used to adjust the frequency of word subsampling.\n",
    "                                   Defaults to 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of keeping each word.\n",
    "    \"\"\"\n",
    "    all_w_count = sum(word_count_dict.values())\n",
    "    freq = {word: word_count_dict[word] / all_w_count for word in word_count_dict}\n",
    "    prob = {word: (threshold / freq[word]) ** 0.5 for word in freq}\n",
    "    return prob\n",
    "\n",
    "def get_negative_sampling_prob(word_count_dict):\n",
    "    \"\"\"Calculates the negative sampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function adjusts the frequency of each word raised to the power of 0.75, which is\n",
    "    commonly used in algorithms like Word2Vec to moderate the influence of very frequent words.\n",
    "    It then normalizes these adjusted frequencies to ensure they sum to 1, forming a probability\n",
    "    distribution used for negative sampling.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of selecting each word\n",
    "            for negative sampling.\n",
    "    \"\"\"\n",
    "    all_w_count = sum(word_count_dict.values())\n",
    "    freq = {word: (word_count_dict[word] / all_w_count) ** 0.75 for word in word_count_dict}\n",
    "    Z = sum(freq.values())\n",
    "    return {word: freq[word] / Z for word in freq}\n",
    "\n",
    "def preprocessing(\n",
    "    data_path: str,\n",
    "    min_count: int = 5,\n",
    "    window_radius: int = 5\n",
    "):\n",
    "    \"\"\"Preprocess data and return different word sampling arrays and dictionaries\n",
    "\n",
    "    Takes in a data directory path and returns context pairs array,\n",
    "    array probabilities of negative sampling and array of probabilities of keeping words\n",
    "\n",
    "    Parameters:\n",
    "    - data_path: Path to data.\n",
    "    - min_count: min number of word occurance in data to add to vocabulary.\n",
    "    - window_radius: number of words to add to the context before and after central word.\n",
    "\n",
    "    Returns:\n",
    "    - word_to_index: mapping of allowed words in data to indexes\n",
    "    - context_pairs: array of tuples (central_word_idx, context_word_idx)\n",
    "    - keep_prob_array: array of probabilities for every allowed word to keep\n",
    "    - negative_sampling_prob_array: array of probabilities for every allowed word\n",
    "        to use as negative sample\n",
    "    \"\"\"\n",
    "    data = list(open(data_path, encoding=\"utf-8\"))\n",
    "\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    data_tok = [\n",
    "        tokenizer.tokenize(\n",
    "            line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "        )\n",
    "        for line in data\n",
    "    ] # генератор в котором токенизируем каждое предложение\n",
    "    data_tok = [x for x in data_tok if len(x) >= 3] # оставляем только те, чьи длина больше 2 (т.е. минимум два слова в предложении)\n",
    "\n",
    "    vocabulary_with_counter = Counter(chain.from_iterable(data_tok))\n",
    "\n",
    "    word_count_dict = dict()\n",
    "    for word, counter in vocabulary_with_counter.items():\n",
    "        if counter >= min_count: # отбрасываем слова встречаемые реже 5 раз\n",
    "            word_count_dict[word] = counter\n",
    "    \n",
    "    vocabulary = set(word_count_dict.keys())\n",
    "    del vocabulary_with_counter\n",
    "\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)} # (слово, индекс)\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()} # (индекс, слово)\n",
    "\n",
    "    context_pairs = []\n",
    "    for text in data_tok:\n",
    "        for i, central_word in enumerate(text): # выбираем центральное слово\n",
    "            context_indices = range(\n",
    "                max(0, i - window_radius), min(i + window_radius, len(text))\n",
    "            ) # сбор контекста к центральному слову\n",
    "            for j in context_indices:\n",
    "                if j == i:\n",
    "                    continue\n",
    "                context_word = text[j]\n",
    "                if central_word in vocabulary and context_word in vocabulary:\n",
    "                    context_pairs.append(\n",
    "                        (word_to_index[central_word], word_to_index[context_word]) # нашли пары разрешенных слов и добавили в массив\n",
    "                    )\n",
    "    keep_prob_dict = subsample_frequent_words(word_count_dict)\n",
    "    negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\n",
    "    # полученные массивы\n",
    "    keep_prob_array = np.array(\n",
    "        [keep_prob_dict[index_to_word[idx]] for idx in range(len(word_to_index))]\n",
    "    )\n",
    "    negative_sampling_prob_array = np.array(\n",
    "        [\n",
    "            negative_sampling_prob_dict[index_to_word[idx]]\n",
    "            for idx in range(len(word_to_index))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return word_to_index, context_pairs, keep_prob_array, negative_sampling_prob_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5f586-a281-4ae0-b9d3-fb4cc6f67bd1",
   "metadata": {},
   "source": [
    "### 03. Создание DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4090cd29-42f4-4f7b-8b65-380de4630d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting skip-gram/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoader for \n",
    "text data.\n",
    "\"\"\"\n",
    "import data_preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import random\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# def get_one_random_sample_with_negatives(context_pairs,\n",
    "#                                          keep_prob_array,\n",
    "#                                          negative_sampling_prob_array,\n",
    "#                                          num_negatives):\n",
    "#     \"\"\"Returns one sample of center, context and negative samples\n",
    "\n",
    "#     Parameters:\n",
    "#     - context_pairs: array of tuples (central_word_idx, context_word_idx)\n",
    "#     - keep_prob_array: array of probabilities for every allowed word to keep\n",
    "#     - negative_sampling_prob_array: array of probabilities for every allowed word\n",
    "#         to use as negative sample\n",
    "\n",
    "#     Returns:\n",
    "#     A tuple of center and context words with negative samples.\n",
    "#     In the form (center, context, neg_sample).\n",
    "#     \"\"\"\n",
    "#     while True:\n",
    "#         center, context = random.choice(context_pairs)\n",
    "#         if random.random() < keep_prob_array[center]:\n",
    "#             neg_sample = np.random.choice(\n",
    "#                 range(len(negative_sampling_prob_array)),\n",
    "#                 size=num_negatives,\n",
    "#                 p=negative_sampling_prob_array,\n",
    "#             )\n",
    "#             return (center, context, neg_sample)\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, context_pairs):\n",
    "        self.context_pairs = context_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.context_pairs[idx]\n",
    "\n",
    "def get_dataloader(\n",
    "    data_path: str,\n",
    "    batch_size: int = 5000,\n",
    "    # num_workers: int=NUM_WORKERS,\n",
    "    num_workers: int=1,\n",
    "    min_count: int = 5,\n",
    "    window_radius: int = 5,\n",
    "    num_negatives: int = 15\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    word_to_index, context_pairs, keep_prob_array, negative_sampling_prob_array = preprocessing(data_path,\n",
    "                                                                                                min_count,\n",
    "                                                                                                window_radius)\n",
    "    dataset = Word2VecDataset(context_pairs)\n",
    "    neg_sampler = WeightedRandomSampler(negative_sampling_prob_array, num_negatives, replacement=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return dataloader, neg_sampler, word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7b503-7498-4c23-96ae-5ef1463d8f06",
   "metadata": {},
   "source": [
    "### 04. Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63ef849f-c2ca-4371-9056-d01ee62bf515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting skip-gram/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a SkipGramModelWithNegSampling model.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramModelWithNegSampling(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModelWithNegSampling, self).__init__()\n",
    "        self.embeddings_in = nn.Embedding(vocab_size, embedding_dim) # center\n",
    "        self.embeddings_out = nn.Embedding(vocab_size, embedding_dim) # context\n",
    "        \n",
    "        # никакая логсигмоида нам не нужна! это все заложено в лоссе\n",
    "        torch.nn.init.xavier_uniform_(self.embeddings_in.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.embeddings_out.weight)\n",
    "        \n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        # center_words — входные слова\n",
    "        # pos_context_words — таргет, т.е. правильный контекст (реально существующий для входного слова)\n",
    "        # neg_context_words — отрицательные примеры — то что не должно быть в контексте\n",
    "\n",
    "        v_in = self.embeddings_in(center_words) \n",
    "        v_out = self.embeddings_out(pos_context_words)\n",
    "        v_neg = self.embeddings_out(neg_context_words)\n",
    "        \n",
    "        pos_scores = (torch.sum(v_in * v_out, dim=1))\n",
    "        neg_scores = (torch.bmm(v_neg, v_in.unsqueeze(2)).squeeze(2)) #.sum(1) # bmm - батчевое (по 2D-матричное) перемножение матриц\n",
    "        return pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6eb83-cbb6-43b0-bebc-9943c2ea5110",
   "metadata": {},
   "source": [
    "### 05. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "570a3eeb-ca76-4934-8232-a67819e33a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting skip-gram/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/engine.py\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    neg_sampler,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler,\n",
    "    device: torch.device,\n",
    "    steps: int,\n",
    "    batch_size: int = 5000,\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    step = 0\n",
    "    loss_history = []\n",
    "    pos_labels = torch.ones(batch_size).to(device)\n",
    "    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n",
    "    while step <= steps:\n",
    "        for target, context in dataloader:\n",
    "            if step > steps:\n",
    "                break\n",
    "            center_words = target.long().to(device)\n",
    "            pos_context_words = context.long().to(device)\n",
    "            neg_context = torch.LongTensor(np.array([np.array(list(neg_sampler)) for t in center_words]))\n",
    "            neg_context_words = neg_context.long().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            pos_scores, neg_scores = model(\n",
    "                center_words, pos_context_words, neg_context_words\n",
    "            )\n",
    "            loss_pos = loss_fn(pos_scores, pos_labels)\n",
    "            loss_neg = loss_fn(neg_scores, neg_labels)\n",
    "    \n",
    "            loss = loss_pos + loss_neg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            loss_history.append(loss.item())\n",
    "            lr_scheduler.step(loss_history[-1])\n",
    "    \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}, Loss: {np.mean(loss_history[-10:])}, learning rate: {lr_scheduler._last_lr}\")\n",
    "            step += 1\n",
    "\n",
    "    return np.mean(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07c74e-ba67-476d-8ffa-af36b794c77c",
   "metadata": {},
   "source": [
    "### 06. Сохранение готовой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4013f4f9-5e71-4951-880d-a3c93414bda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing skip-gram/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/utils.py\n",
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "    Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"skip-gram with negative sampling.pth\")\n",
    "    \"\"\"\n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c18698-6f0c-46dc-8806-bdcd13ba203d",
   "metadata": {},
   "source": [
    "### 07. Обучение модели, проверка и сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff969d2f-3be7-4d70-b0b3-da7e5e4765f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, neg_sampler, word_to_index = get_dataloader('data/quora.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c61e555-7a20-4fe4-9c02-5ab7c5f8a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting skip-gram/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skip-gram/train.py\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torch.optim as optim\n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_STEPS = 750\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 32\n",
    "num_negatives = 15\n",
    "model_0 = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss() # тот самый лосс, похож на логлосс\n",
    "optimizer = optim.Adam(model_0.parameters(), lr=0.05)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=30) # штука, которая будет уполовинивать (* factor = 1/2) lr при отсутствии улучшений в течение 150 эпох\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_0_results = train(model=model_0, \n",
    "                        dataloader=dataloader,\n",
    "                        neg_sampler=neg_sampler,\n",
    "                        loss_fn=loss_fn, \n",
    "                        optimizer=optimizer,\n",
    "                        lr_scheduler=lr_scheduler,\n",
    "                        steps=NUM_STEPS,\n",
    "                        device=device)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "save_model(model=model_0,\n",
    "           target_dir=\"models\",\n",
    "           model_name=\"skip-gram with negative sampling.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e504721c-5181-4e3f-813e-924538cc2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 1.178739309310913, learning rate: [0.003125]\n",
      "Step 10, Loss: 1.1792627811431884, learning rate: [0.003125]\n",
      "Step 20, Loss: 1.1801915049552918, learning rate: [0.003125]\n",
      "Step 30, Loss: 1.1780844688415528, learning rate: [0.003125]\n",
      "Step 40, Loss: 1.1769014477729798, learning rate: [0.0015625]\n",
      "Step 50, Loss: 1.1806744456291198, learning rate: [0.0015625]\n",
      "Step 60, Loss: 1.1832282185554504, learning rate: [0.0015625]\n",
      "Step 70, Loss: 1.180414652824402, learning rate: [0.00078125]\n",
      "Step 80, Loss: 1.1816535592079163, learning rate: [0.00078125]\n",
      "Step 90, Loss: 1.1767923831939697, learning rate: [0.00078125]\n",
      "Step 100, Loss: 1.1825859546661377, learning rate: [0.000390625]\n",
      "Step 110, Loss: 1.177309775352478, learning rate: [0.000390625]\n",
      "Step 120, Loss: 1.1782177329063415, learning rate: [0.000390625]\n",
      "Step 130, Loss: 1.176673173904419, learning rate: [0.000390625]\n",
      "Step 140, Loss: 1.1798057556152344, learning rate: [0.0001953125]\n",
      "Step 150, Loss: 1.1756161451339722, learning rate: [0.0001953125]\n",
      "Step 160, Loss: 1.1801307797431946, learning rate: [0.0001953125]\n",
      "Step 170, Loss: 1.1786084055900574, learning rate: [9.765625e-05]\n",
      "Step 180, Loss: 1.1769997119903564, learning rate: [9.765625e-05]\n",
      "Step 190, Loss: 1.1773235559463502, learning rate: [9.765625e-05]\n",
      "Step 200, Loss: 1.1782099723815918, learning rate: [4.8828125e-05]\n",
      "Step 210, Loss: 1.175533366203308, learning rate: [4.8828125e-05]\n",
      "Step 220, Loss: 1.1770228743553162, learning rate: [4.8828125e-05]\n",
      "Step 230, Loss: 1.177378284931183, learning rate: [2.44140625e-05]\n",
      "Step 240, Loss: 1.1783034682273865, learning rate: [2.44140625e-05]\n",
      "Step 250, Loss: 1.1772818088531494, learning rate: [2.44140625e-05]\n",
      "Step 260, Loss: 1.1789188027381896, learning rate: [2.44140625e-05]\n",
      "Step 270, Loss: 1.1744940280914307, learning rate: [1.220703125e-05]\n",
      "Step 280, Loss: 1.1779721617698669, learning rate: [1.220703125e-05]\n",
      "Step 290, Loss: 1.1798027992248534, learning rate: [1.220703125e-05]\n",
      "Step 300, Loss: 1.178540587425232, learning rate: [6.103515625e-06]\n"
     ]
    }
   ],
   "source": [
    "model_0_results = train(model=model_0, \n",
    "                        dataloader=dataloader,\n",
    "                        neg_sampler=neg_sampler,\n",
    "                        loss_fn=loss_fn, \n",
    "                        optimizer=optimizer,\n",
    "                        lr_scheduler=lr_scheduler,\n",
    "                        steps=NUM_STEPS,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff7082d9-2d61-4431-b05e-edf42647bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {i : w for w, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebb2c23d-cd7f-4724-aaad-67f3a2c2c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = model_0.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d01a884d-3b14-4e9a-b26c-c907898e01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, embedding_matrix, word_to_index=word_to_index):\n",
    "    return embedding_matrix[word_to_index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40d8fb0f-8697-4b63-9c86-d476b0c17857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_nearest(word, embedding_matrix, word_to_index=word_to_index, k=10):\n",
    "    word_vector = get_word_vector(word, embedding_matrix)[None, :]\n",
    "    dists = F.cosine_similarity(embedding_matrix, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-k:]\n",
    "    return [(index_to_word[x], dists[x].item()) for x in top_k.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92b2f147-3a05-4bea-bdc8-6c46003ebe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ios', 0.7597508430480957),\n",
       " ('pointers', 0.7638192772865295),\n",
       " ('matlab', 0.7749127745628357),\n",
       " ('linux', 0.7759209275245667),\n",
       " ('html', 0.7836021780967712),\n",
       " ('javascript', 0.7920598387718201),\n",
       " ('c', 0.8066774606704712),\n",
       " ('programming', 0.8400071263313293),\n",
       " ('java', 0.8756576180458069),\n",
       " ('python', 1.0000001192092896)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(\"python\", embedding_matrix_context, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da014d-dcc7-470b-8098-a415cce7bec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
